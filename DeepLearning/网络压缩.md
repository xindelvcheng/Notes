

# 深度学习笔记

[TOC]

## 一.网络压缩

### 1.网络剪枝（Network Pruning）

网络剪枝是指去掉网络中一些输出总是接近于0对结果没有贡献的节点（理论上也可以去除Weight，但因为无法被GPU加速的原因没有实用性）。从生物学来看，脊椎动物的神经系统在发育过程中，约有50%的细胞都凋亡了。

那么为什么不一开始就选择训练一个或一些小网络呢？可能是因为实践中发现大网络更容易训练（有论文说 明了这点，也有论文反驳了这一点），而且最开始也不知道多大的网络可以完成任务。

### 2.知识蒸馏（Knowledge Distillation）

> 这个方法似乎没什么用……

使用训练集的数据和标签训练大网络，使用训练集的数据和大网络的输出训练小网络，可能是因为大网络的输出因为有置信度表明分类之间的联系相对标签含有更多的信息。实践中，可以修改softmax从$y_i = \frac{exp(x_i)}{\sum{exp(x_j)}}$为$y_i = \frac{exp(x_i/T)}{\sum{exp(x_i/T)}}$（实做不需要修改softmax只需要传入x时除T），将其中的输入除一个数让各个分类的置信度差距变小（否则非0即1和标签没有区别）。代码如下：



### 3.参数量化（Parameter Quantization）

神经网络网络中的参数用不到32bit Float那么大的表示范围，如果使用8bit来表示，模型就缩小为四分之一。

