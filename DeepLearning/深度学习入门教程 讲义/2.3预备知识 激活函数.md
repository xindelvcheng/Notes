# 3.3 预备知识 激活函数

我们知道神经网络就是由一堆矩阵构成的一个巨型函数，输入x经过每一层都是进行

$x = Wx+b$

的运算。

![image-20200624114025702](image-20200624114025702.png)

但是只是单纯地叠多层并没有什么意义，因为在神经网络中，每一层输出的都是上一层输入的线性函数，所以无论网络结构怎么搭，输出都是输入的**线性组合**。

![这里写图片描述](7c6e12aed30bf315eed8df6476d7ef7b_b.png)

所以，我们需要在传递下一层数据之前，加上一个激活函数

![simoid tanh](241221240623467.png)

这样神经网络的每一层就变成了
$$
y = \sigma (Wx+b)
$$
两层就是
$$
y = \sigma(W_2\sigma (W_1x+b_1)+b_2)
$$
这样深度的神经网络才有意义。

## 常见的激活函数

### 1.Relu函数

$$
	\left\{  
             \begin{array}{**lr**}  
             y=x & x>=0 \\  
             y=0, & x<0.\\  
                
             \end{array}  
\right.
$$





![这里写图片描述](20180503231727530)

### 2.双曲正切

$$
tanh(x) = \frac{e^x-e^{-x}}{e^x+e^{-x}}
$$



![tanh(x)及其导数的几何图像](2018041517590341)

### 3.Softmax

$$
S_i = \frac{e^i}{\sum_je^j}
$$

![这里写图片描述](20180902220822202)

用于分类问题

![img](d62a6059252dd42af3835f580f3b5bb5c8eab8bf)

## Pytorch中的激活函数

### 