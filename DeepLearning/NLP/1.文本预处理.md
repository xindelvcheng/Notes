# 文本预处理

[TOC]

## 一.分词

### 1.jieba

```python
jieba.lcut()
```

### 2.hanlp

基于深度学习的分词。

安装命令如下：

```bash
pip install hanlp[full]
```

使用方式为可调用对象，代码如下：

```python
import hanlp

text = "hanlp是面向生产环境的多语种自然语言处理工具包，基于PyTorch和TensorFlow 2.x双引擎，目标是普及落地最前沿的NLP技术。"
toeknizer = hanlp.load("CTB6_CONVSEG")
print(toeknizer(text))
```

## 二.实体识别

使用hanlp库，加载模型（中文为`hanlp.pretrained.ner.MSRA_NER_BERT_BASE_Z`，英文为`hanlp.pretrained.ner.CONLL03_NER_BERT_BASE_UNCASED_EN`）以可调用对象的方式，代码如下：

```python
import hanlp

text = "自2015年北京申冬奥成功以来，作为反映盛会筹办进展的一面镜子，竞赛场馆建设工作备受世人关注。"
recognizer = hanlp.load(hanlp.pretrained.ner.MSRA_NER_BERT_BASE_ZH)
print(recognizer(list(text)))
```

## 三.词性标注

Part-Of-Speech tagging，简称POS，即标注出文本中每个词汇的词性。

### 1.jieba

```python
import jieba.posseg as pseg
text = "自2015年北京申冬奥成功以来，作为反映盛会筹办进展的一面镜子，竞赛场馆建设工作备受世人关注。"
print(pseg.lcut(text))
```

### 2.hanlp

```python
import jieba
import hanlp

text = "自2015年北京申冬奥成功以来，作为反映盛会筹办进展的一面镜子，竞赛场馆建设工作备受世人关注。"
tagger = hanlp.load(hanlp.pretrained.pos.CTB5_POS_RNN_FASTTEXT_ZH)
print(tagger(jieba.lcut(text)))
```

## 四.文本编码

将文本转为张量，以便输入神经网络。

### 1.One-Hot

```python
import jieba
import torch

text = "hanlp是面向生产环境的多语种自然语言处理工具包，基于PyTorch和TensorFlow 2.x双引擎，目标是普及落地最前沿的NLP技术。"
vocab = set(jieba.lcut(text))

index = 0
n = len(vocab)
one_hot_map = {}
for word in vocab:
    tensor = torch.zeros(n)
    tensor[index] = 1
    index += 1
    one_hot_map[word] = tensor

print(one_hot_map["自然语言"])
```

该方法浪费空间且不携带词义，很少使用。

### 2.word2vec

##### ①准备语料

Windows下直接访问网址下载并用解压软件解压。Linux下可以使用wget命令，命令如下：

```bash
$ wget -c http://mattmahoney.net/dc/enwik9.zip -P ./data
```

-c：断点续传

-P：下载到指定文件夹

解压命令如下：

```bash
$ unzip ./data/enwik9.zip -d ./data
```

-d：解压到指定文件夹

##### ②使用脚本从HTML中提取数据

从[fastText/wikifil.pl at master · facebookresearch/fastText · GitHub](https://github.com/facebookresearch/fastText/blob/master/wikifil.pl)下载wikifil.pl，处理HTML数据。

```bash
$ wget -c https://raw.githubusercontent.com/facebookresearch/fastText/master/wikifil.pl
$ perl wikifil.pl ./data/enwik9 > ./data/fil9
```

##### ③训练词向量

使用fasttext包

```python
import fasttext

model = fasttext.train_unsupervised("./fil9")
```

当不设定任何超参数时，默认使用skipgram模式，词嵌入维度为100，epoch为5，学习率为0.05，线程数为12。

##### ④测试模型

```python
model.get_word_vector("cat")
```

##### ⑤模型保存和加载

```python
model.save_model("fil9.bin")
model = fasttext.load_model("fil9.bin")
```

### 3.WordEmbedding

广义的WordEmbedding包含各种词汇向量的表示方法，包含word2vec，狭义的WordEmbedding指在网络进行训练时产生的Embedding矩阵。

## 五.文本数据分析

文本数据分析是通过统计方法 分析语料的特点，帮助选择模型超参数。

主要使用库seaborn，它是基于matplotlib的数据可视化库，提供更高级的API。

###  1.数据标签分布

##### ①对连续变量：使用pyplot.hist或seaborn.distplot

seaborn.displot()，统计不同区间所占的比重（countplot统计单个值），通常是对连续变量使用

```python
data = [1, 1, 2, 5, 6]
seaborn.displot(data=data)
```

##### ② 对离散变量:使用countplot

seaborn.countplot()，统计某列不同的值所占的比重

```python
train_data = pd.read_csv("./data/cn_data/train.tsv", sep="\t")
seaborn.countplot(x="label", data=train_data)
```

##### ③判断异常点：使用stripplot

画出散点图，可以很容易看出异常点

### 2.词云

需要使用wordcloud包

## 六.文本数据处理

### 1.长度规范

使用keras.preprocessing.sequence.pad_sequences()函数。

```python
from keras.preprocessing import sequence

unified_sentence_length = 10
sentence = [[1, 2, 3, 4], [1, 2, 3]]
sentence = sequence.pad_sequences(sentence, maxlen=unified_sentence_length)
```

